#!/usr/bin/env python2
from sklearn.cluster import KMeans
from sklearn.cluster import SpectralClustering
from sklearn.feature_selection import VarianceThreshold
from multiprocessing import Pool
import numpy as np
import argparse
import json
import sys

def cluster_kmeans(args, index_to_file, vecs, sel=None):
  na = np.array(vecs)
  if sel != None:
    na = sel.fit_transform(na)
  kmeans = KMeans(n_clusters=args.clusters, n_jobs=-1).fit(na)
  labels = []
  for i,c in enumerate(kmeans.labels_):
    labels.append((str(index_to_file[i]), int(c)))
  results = {}
  results['labels'] = labels
  results['name'] = 'kmeans'
  return results 

def cluster_spectral(args, index_to_file, vecs, sel=None):
  na = np.array(vecs)
  if sel != None:
    na = sel.fit_transform(na)
  spec = SpectralClustering(n_clusters=args.clusters, n_jobs=-1).fit(na)
  labels = []
  for i,c in enumerate(spec.labels_):
    labels.append((str(index_to_file[i]), int(c)))
  results = {}
  results['labels'] = labels
  results['name'] = 'spectral'
  return results 

def main(args):
  # Read in all the vector files as dictionaries. 
  
  map_dicts = []
  file_to_index = {}
  index_to_file = {}
  i = 0
  for f in args.files:
    r = json.load(open(f, 'r'))
    map_dicts.append(r['tuples'])
    file_to_index[r['file']] = i
    index_to_file[i] = r['file']
    i = i + 1

  # Pad out all the dicts in map.dicts to have the same key set.
  total_key_set = set()
  for m in map_dicts:
    total_key_set = total_key_set.union(m.keys())
  for m in map_dicts:
    for k in total_key_set:
      if k not in m.keys():
        m[k] = 0

  # Now, turn these into actual vectors, with an order controlled by
  # the set of all keys. 
  vecs = []
  for m in map_dicts:
    vec = []
    for k in total_key_set:
      vec.append(m[k])
    vecs.append(vec)
  if args.delog:
    # Our feature vectors are log scaled, and we're asked to go in and de-log them. 
    # Raise each to the 2nd power.
    for v in vecs:
      for i in range(0, len(v)):
        v[i] = v[i]**2
  # Now we can actually do some clustering. 
  cluster_meth = None
  if args.cluster_method == 'kmeans':
    cluster_meth = cluster_kmeans
  elif args.cluster_method == 'spectral':
    cluster_meth = cluster_spectral
  else:
    print "Unknown cluster method %s" % args.cluster_method
    return 1

  sel = None
  if args.variance_threshold != None:
    sel = VarianceThreshold(threshold=(args.variance_threshold * (1-args.variance_threshold)))
  results = cluster_meth(args, index_to_file, vecs, sel)
  # Write the results of this clustering out to a file. 
  out = open(args.outfile, 'w')
  json.dump(results, out)
  out.close()
  return 0

if __name__ == '__main__':
  parser = argparse.ArgumentParser("cluster")
  parser.add_argument('--name', type=str)
  parser.add_argument('--outfile', type=str, default='cluster.json')
  parser.add_argument('files', nargs='+')
  parser.add_argument('--clusters', type=int, default=8)
  parser.add_argument('--delog', default=False, action='store_true')
  parser.add_argument('--variance-threshold', default=None, type=float)
  parser.add_argument('--cluster-method', type=str, default='kmeans', choices=['kmeans', 'spectral'])
  args = parser.parse_args()
  sys.exit(main(args))

